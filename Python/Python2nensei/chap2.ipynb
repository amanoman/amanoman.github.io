{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "\n",
      "<html>\n",
      "<head>\n",
      "<meta charset=\"utf-8\"/>\n",
      "<title>Python2年生</title>\n",
      "</head>\n",
      "<body>\n",
      "<h2>第1章 Pythonでデータをダウンロード</h2>\n",
      "<ol>\n",
      "<li>スクレイピングってなに？</li>\n",
      "<li>Pythonをインストールしてみよう</li>\n",
      "<li>requestsでアクセスしてみよう</li>\n",
      "</ol>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Webページを取得して解析する\n",
    "load_url = \"https://www.ymori.com/books/python2nen/test1.html\"\n",
    "html = requests.get(load_url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "\n",
    "#html全体を表示する\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Python2年生</title>\n",
      "<h2>第1章 Pythonでデータをダウンロード</h2>\n",
      "<li>スクレイピングってなに？</li>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "load_url = \"https://www.ymori.com/books/python2nen/test1.html\"\n",
    "html = requests.get(load_url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "\n",
    "#title,h2,liタグを検索して表示する\n",
    "print(soup.find(\"title\"))\n",
    "print(soup.find(\"h2\"))\n",
    "print(soup.find(\"li\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python2年生\n",
      "第1章 Pythonでデータをダウンロード\n",
      "スクレイピングってなに？\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "load_url = \"https://www.ymori.com/books/python2nen/test1.html\"\n",
    "html = requests.get(load_url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "\n",
    "#title,h2,liタグを検索して表示する\n",
    "print(soup.find(\"title\").text)\n",
    "print(soup.find(\"h2\").text)\n",
    "print(soup.find(\"li\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "スクレイピングってなに？\n",
      "Pythonをインストールしてみよう\n",
      "requestsでアクセスしてみよう\n",
      "HTMLを解析してみよう\n",
      "ニュースの最新記事一覧を取得してみよう\n",
      "リンク一覧をファイルに書き出そう\n",
      "画像を一括ダウンロードしよう\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "load_url = \"https://www.ymori.com/books/python2nen/test2.html\"\n",
    "html = requests.get(load_url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "\n",
    "#すべてのliタグを県sなくして、その文字列を表示する\n",
    "for element in soup.find_all(\"li\"):\n",
    "    print(element.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div id=\"chap2\">\n",
      "<h2>第2章 HTMLを解析しよう</h2>\n",
      "<ol>\n",
      "<li>HTMLを解析してみよう</li>\n",
      "<li>ニュースの最新記事一覧を取得してみよう</li>\n",
      "<li>リンク一覧をファイルに書き出そう</li>\n",
      "<li>画像を一括ダウンロードしよう</li>\n",
      "</ol>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "load_url = \"https://www.ymori.com/books/python2nen/test2.html\"\n",
    "html = requests.get(load_url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "\n",
    "#IDで検索して、そのタグの中身を表示する\n",
    "chap2 = soup.find(id=\"chap2\")\n",
    "print(chap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTMLを解析してみよう\n",
      "ニュースの最新記事一覧を取得してみよう\n",
      "リンク一覧をファイルに書き出そう\n",
      "画像を一括ダウンロードしよう\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "load_url = \"https://www.ymori.com/books/python2nen/test2.html\"\n",
    "html = requests.get(load_url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "\n",
    "#IDで検索して、その中のすべてのliタグを検索して表示する\n",
    "chap2 = soup.find(id=\"chap2\")\n",
    "for element in chap2.find_all(\"li\"):\n",
    "    print(element.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PS5 来年12月4日発売はデマ\n",
      "Office365 障害にみるリスク\n",
      "電子マネーさい銭 課税ある?\n",
      "群馬で検索上昇「群馬電機」\n",
      "漫画海賊版サイト 今も500超\n",
      "ネットバンク 不正送金急増\n",
      "#忘年会スルー 世代超え共感\n",
      "スマホで自動翻訳 Google提供\n",
      "もっと見る\n",
      "PS5 来年12月4日発売はデマ\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "load_url = \"https://news.yahoo.co.jp/categories/it\"\n",
    "html = requests.get(load_url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "\n",
    "#IDで検索して、そのタグの中身を表示する\n",
    "topic = soup.find(class_=\"topics\")\n",
    "for element in topic.find_all(\"a\"):\n",
    "    print(element.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PS5 来年12月4日発売はデマ\n",
      "https://news.yahoo.co.jp/pickup/6345463\n",
      "Office365 障害にみるリスク\n",
      "https://news.yahoo.co.jp/pickup/6345383\n",
      "電子マネーさい銭 課税ある?\n",
      "https://news.yahoo.co.jp/pickup/6345375\n",
      "群馬で検索上昇「群馬電機」\n",
      "https://news.yahoo.co.jp/pickup/6345348\n",
      "漫画海賊版サイト 今も500超\n",
      "https://news.yahoo.co.jp/pickup/6345336\n",
      "ネットバンク 不正送金急増\n",
      "https://news.yahoo.co.jp/pickup/6345315\n",
      "#忘年会スルー 世代超え共感\n",
      "https://news.yahoo.co.jp/pickup/6345250\n",
      "スマホで自動翻訳 Google提供\n",
      "https://news.yahoo.co.jp/pickup/6345222\n",
      "もっと見る\n",
      "/topics/it\n",
      "PS5 来年12月4日発売はデマ\n",
      "https://news.yahoo.co.jp/pickup/6345463\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "load_url = \"https://news.yahoo.co.jp/categories/it\"\n",
    "html = requests.get(load_url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "\n",
    "#IDで検索して、そのタグの中身を表示する\n",
    "topic = soup.find(class_=\"topics\")\n",
    "for element in topic.find_all(\"a\"):\n",
    "    print(element.text)\n",
    "    url = element.get(\"href\")\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PS5 来年12月4日発売はデマ\n",
      "https://news.yahoo.co.jp/pickup/6345463\n",
      "Office365 障害にみるリスク\n",
      "https://news.yahoo.co.jp/pickup/6345383\n",
      "電子マネーさい銭 課税ある?\n",
      "https://news.yahoo.co.jp/pickup/6345375\n",
      "群馬で検索上昇「群馬電機」\n",
      "https://news.yahoo.co.jp/pickup/6345348\n",
      "漫画海賊版サイト 今も500超\n",
      "https://news.yahoo.co.jp/pickup/6345336\n",
      "ネットバンク 不正送金急増\n",
      "https://news.yahoo.co.jp/pickup/6345315\n",
      "#忘年会スルー 世代超え共感\n",
      "https://news.yahoo.co.jp/pickup/6345250\n",
      "スマホで自動翻訳 Google提供\n",
      "https://news.yahoo.co.jp/pickup/6345222\n",
      "もっと見る\n",
      "https://news.yahoo.co.jp/topics/it\n",
      "PS5 来年12月4日発売はデマ\n",
      "https://news.yahoo.co.jp/pickup/6345463\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "load_url = \"https://news.yahoo.co.jp/categories/it\"\n",
    "html = requests.get(load_url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "\n",
    "#IDで検索して、そのタグの中身を表示する\n",
    "topic = soup.find(class_=\"topics\")\n",
    "for element in topic.find_all(\"a\"):\n",
    "    print(element.text)\n",
    "    url = element.get(\"href\")\n",
    "    link_url = urllib.parse.urljoin(load_url, url)\n",
    "    print(link_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "load_url = \"https://www.ymori.com/books/python2nen/test2.html\"\n",
    "html = requests.get(load_url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "\n",
    "#ファイルを書き込みモードで開く\n",
    "filename = \"linklist.txt\"\n",
    "with open(filename, \"w\") as f:\n",
    "    #すべてのaタグを検索し、リンクを絶対URLで書き出す\n",
    "    for element in topic.find_all(\"a\"):\n",
    "        url = element.get(\"href\")\n",
    "        link_url = urllib.parse.urljoin(load_url, url)\n",
    "        f.write(element.text+\"\\n\")\n",
    "        f.write(link_url+\"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "#画像ファイルを取得する\n",
    "image_url = \"https://www.ymori.com/books/python2nen/sample1.png\"\n",
    "imgdata = requests.get(image_url)\n",
    "\n",
    "#URLから最後のファイル名を取り出す\n",
    "filename = image_url.split(\"/\")[-1]\n",
    "\n",
    "#画像データをファイルに書き出す\n",
    "with open(filename, mode=\"wb\") as f:\n",
    "    f.write(imgdata.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "#保存用フォルダを作る\n",
    "out_folder = Path(\"download\")\n",
    "out_folder.mkdir(exist_ok=True)\n",
    "\n",
    "#画像ファイルを取得する\n",
    "image_url = \"https://www.ymori.com/books/python2nen/sample1.png\"\n",
    "imgdata = requests.get(image_url)\n",
    "\n",
    "#URLから最後のファイル名を取り出して、保存フォルダ名とつなげる\n",
    "filename = image_url.split(\"/\")[-1]\n",
    "out_path = out_folder.joinpath(filename)\n",
    "\n",
    "\n",
    "#画像データをファイルに書き出す\n",
    "with open(out_path, mode=\"wb\") as f:\n",
    "    f.write(imgdata.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ymori.com/books/python2nen/sample1.png >> sample1.png\n",
      "https://www.ymori.com/books/python2nen/sample2.png >> sample2.png\n",
      "https://www.ymori.com/books/python2nen/sample3.png >> sample3.png\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "#webページを取得して解析する\n",
    "load_url = \"https://www.ymori.com/books/python2nen/test2.html\"\n",
    "html = requests.get(load_url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "\n",
    "#すべてのimgタグを検索し、リンクを取得する\n",
    "for element in soup.find_all(\"img\"):\n",
    "    src = element.get(\"src\")\n",
    "    \n",
    "    #絶対URLとファイルを表示する\n",
    "    image_url = urllib.parse.urljoin(load_url, src)\n",
    "    filename = image_url.split(\"/\")[-1]\n",
    "    print(image_url, \">>\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import urllib\n",
    "import time\n",
    "\n",
    "#webページを取得して解析する\n",
    "load_url = \"https://www.ymori.com/books/python2nen/test2.html\"\n",
    "html = requests.get(load_url)\n",
    "soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "\n",
    "#保存用フォルダを作る\n",
    "out_folder = Path(\"download2\")\n",
    "out_folder.mkdir(exist_ok=True)\n",
    "\n",
    "#すべてのimgタグを検索し、リンクを取得する\n",
    "for element in soup.find_all(\"img\"):\n",
    "    src = element.get(\"src\")\n",
    "    \n",
    "    #絶対URLとファイルを表示する\n",
    "    image_url = urllib.parse.urljoin(load_url, src)\n",
    "    imgdata = requests.get(image_url)\n",
    "    \n",
    "    #URLから最後のファイル名を取り出して保存フォルダ名とつなげる\n",
    "    filename = image_url.split(\"/\")[-1]\n",
    "    out_path = out_folder.joinpath(filename)\n",
    "    \n",
    "    #画像データをファイルに書き出す\n",
    "    with open(out_path, mode=\"wb\") as f:\n",
    "        f.write(imgdata.content)\n",
    "        \n",
    "    #１回アクセスしたので１秒まつ\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
